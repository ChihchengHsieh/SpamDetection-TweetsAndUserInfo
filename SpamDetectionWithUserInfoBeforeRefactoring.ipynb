{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from Constants import Constants, specialTokenList, specialTokens\n",
    "from All_Models import SSCL, GatedCNN, SelfAttnModel\n",
    "from utils import getSampler\n",
    "from LoadData import loadingData\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "'''\n",
    "TODO:\n",
    "\n",
    "1. Mardularize \n",
    "2. Pickle the output\n",
    "3. Training\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Using inplace method for the dat\n",
    "\n",
    "class args(object):\n",
    "\n",
    "    # Data\n",
    "    \n",
    "    \n",
    "    dataset = \"HSpam14\"\n",
    "    full_data = True\n",
    "    usingWeightRandomSampling = True\n",
    "    vocab_size = 8000 # if we create the new vocab size, we have to do the new preprocess again\n",
    "    validation_portion = 0.05\n",
    "    test_portion = 0.04\n",
    "    random_seed = 64\n",
    "    \n",
    "    pickle_name = \"UserAndTweetsFullPickleData\"+ str(vocab_size) + \"Vocab.txt\"\n",
    "    pickle_name_beforeMapToIdx = \"UserAndTweetsFullPickleDatabeforeMapToIdx.txt\"\n",
    "\n",
    "    \n",
    "    ### Preprocessing Data\n",
    "\n",
    "    \n",
    "\n",
    "    ##### Arch\n",
    "    \n",
    "\n",
    "\n",
    "    ## GatedCNN arch\n",
    "\n",
    "    GatedCNN_embedingDim = 128\n",
    "    GatedCNN_convDim = 64\n",
    "    GatedCNN_kernel = 3\n",
    "    GatedCNN_stride = 1\n",
    "    GatedCNN_pad = 1\n",
    "    GatedCNN_layers = 8\n",
    "    GatedCNN_dropout = 0.1\n",
    "        \n",
    "    ## SSCL arch\n",
    "    \n",
    "    \n",
    "    SSCL_embedingDim = 512\n",
    "    SSCL_RNNHidden = 256\n",
    "    SSCL_CNNDim = 256\n",
    "    SSCL_CNNKernel = 5\n",
    "    SSCL_CNNDropout = 0.1\n",
    "    SSCL_LSTMDropout = 0.1\n",
    "    SSCL_LSTMLayers = 1\n",
    "    \n",
    "    ## Attn arch\n",
    "\n",
    "    SelfAttn_LenMaxSeq = 280 # Default, will be changed Later\n",
    "\n",
    "    # These Two has to be the same\n",
    "    SelfAttn_WordVecDim = 128\n",
    "    SelfAttn_ModelDim = 128\n",
    "    \n",
    "    SelfAttn_FFInnerDim = 256\n",
    "    SelfAttn_NumLayers = 3\n",
    "    SelfAttn_NumHead = 4\n",
    "    SelfAttn_KDim = 64\n",
    "    SelfAttn_VDim = 64\n",
    "    SelfAttn_Dropout = 0.1\n",
    "    \n",
    "    \n",
    "    ## MultiTask Model\n",
    "    \n",
    "    FC_hidden = 16\n",
    "    \n",
    "    \n",
    "    # Training params\n",
    "    \n",
    "    batch_size = 64\n",
    "    L2 = 0.1\n",
    "    threshold = 0.5\n",
    "    lr = 0.002\n",
    "    n_epoch = 50\n",
    "\n",
    "    # If using Adam\n",
    "    adam_beta1 = 0.9\n",
    "    adam_beta2 = 0.999\n",
    "    \n",
    "    earlyStopStep = 50000 # Set None if we don't want it\n",
    "    earlyStopEpoch = 200 #\n",
    "\n",
    "    # Logging the Training\n",
    "    val_freq = 50\n",
    "    val_steps = 3\n",
    "    log_freq = 10\n",
    "    model_save_freq = 1\n",
    "    model_name = 'TestingModel'\n",
    "    model_path = './'+ dataset +'_Log/' + model_name + '/Model/'\n",
    "    log_path = './' + dataset +'_Log/' + model_name + '/Log/'\n",
    "    \n",
    "args.device = device\n",
    "\n",
    "# Create the path for saving model and the log\n",
    "if not os.path.exists(args.model_path):\n",
    "    os.makedirs(args.model_path)\n",
    "\n",
    "if not os.path.exists(args.log_path):\n",
    "    os.makedirs(args.log_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import preprocessingInputData, matchingURL, mapFromWordToIdx, CreateDatatset\n",
    "import itertools\n",
    "import pickle\n",
    "from Constants import specialTokenList\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_html(os.path.join(args.dataset, \"2thTweetsWithUserInfoSelected.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>numberOfHashtags_c</td>\n",
       "      <td>favorite_count</td>\n",
       "      <td>retweet_count</td>\n",
       "      <td>maliciousMark</td>\n",
       "      <td>possibly_sensitive</td>\n",
       "      <td>followers_count</td>\n",
       "      <td>friends_count</td>\n",
       "      <td>default_profile</td>\n",
       "      <td>default_profile_image</td>\n",
       "      <td>favourites_count</td>\n",
       "      <td>listed_count</td>\n",
       "      <td>statuses_count</td>\n",
       "      <td>verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>That just seriously ruined my night..</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>173</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>562</td>\n",
       "      <td>2</td>\n",
       "      <td>22545</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@JessleaC I no they are so funny ..! I love yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129</td>\n",
       "      <td>202</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1131</td>\n",
       "      <td>1</td>\n",
       "      <td>2192</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@honkwas might drive so we could go subway or ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>207</td>\n",
       "      <td>372</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1066</td>\n",
       "      <td>1</td>\n",
       "      <td>3426</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @4dize: my friends keep tellin me, that if ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6218</td>\n",
       "      <td>915</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>513</td>\n",
       "      <td>44</td>\n",
       "      <td>223046</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>That @brad_frost post is bourne out by my expe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282</td>\n",
       "      <td>471</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5452</td>\n",
       "      <td>54</td>\n",
       "      <td>5378</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>why tf do i keep waking up every 2 hours -.-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>836</td>\n",
       "      <td>415</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1814</td>\n",
       "      <td>2</td>\n",
       "      <td>12523</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I love playing a girl Fav song &amp; watch them go...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3301</td>\n",
       "      <td>1113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "      <td>26644</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Yup late bout to write some music. Working on ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>444</td>\n",
       "      <td>331</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11964</td>\n",
       "      <td>3</td>\n",
       "      <td>10852</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @2PM_facts: RT @jypnation [2PM LIVE TOUR in...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>451</td>\n",
       "      <td>277</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>24706</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i be up all night whole crew on here. cuz i do...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>1469</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@FGW passenger fainted on 734am bristol glouce...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87</td>\n",
       "      <td>271</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>1189</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Make a change from other artist and actually t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79222</td>\n",
       "      <td>37027</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2144</td>\n",
       "      <td>142</td>\n",
       "      <td>56272</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>My 2nd pair of ghds are on there way out??</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>334</td>\n",
       "      <td>278</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2531</td>\n",
       "      <td>0</td>\n",
       "      <td>17858</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"@TheBlogz: @K_B_Production nigga. You need yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1426</td>\n",
       "      <td>647</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6555</td>\n",
       "      <td>13</td>\n",
       "      <td>30125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@DanielleLopez14 @BritttanyBetchh and now Case...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2314</td>\n",
       "      <td>0</td>\n",
       "      <td>2742</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Where words fails, music Speaks !</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RT @1Obefiend: So i heard Milo Suam kena tangk...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>886</td>\n",
       "      <td>587</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>730</td>\n",
       "      <td>9</td>\n",
       "      <td>82058</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>If you ever sleep with @MilaKunis you'd have t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1067</td>\n",
       "      <td>1742</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1430</td>\n",
       "      <td>14</td>\n",
       "      <td>8877</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Thankyou so much you beautiful boy. You are th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>727</td>\n",
       "      <td>967</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5610</td>\n",
       "      <td>0</td>\n",
       "      <td>20205</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>“@cynthia3950: @ABQuintanilla3 Thank u soo muc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88</td>\n",
       "      <td>1072</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>349</td>\n",
       "      <td>1</td>\n",
       "      <td>1245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RT @AndieForbes_xx: I love the south African a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>368</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1061</td>\n",
       "      <td>0</td>\n",
       "      <td>32569</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RT @WSJ: Today's smartphones have roughly the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>304</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RT @__Skully: I love how angry my brother gets...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>742</td>\n",
       "      <td>596</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5777</td>\n",
       "      <td>0</td>\n",
       "      <td>11320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I've collected 12,293 gold coins! http://t.co/...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13134</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RT @FIirting: It's amazing how you can fall in...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1115</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>416</td>\n",
       "      <td>396</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9666</td>\n",
       "      <td>1</td>\n",
       "      <td>52208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RT @carolekirk: One week to go! 9th May 11-3 t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2027</td>\n",
       "      <td>2471</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>831</td>\n",
       "      <td>85</td>\n",
       "      <td>8020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>@HaleyMatthew02 how were u playing on PokerSta...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1340</td>\n",
       "      <td>867</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1233</td>\n",
       "      <td>24</td>\n",
       "      <td>22959</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>people that nickname their first name on twitt...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>971</td>\n",
       "      <td>696</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10110</td>\n",
       "      <td>4</td>\n",
       "      <td>10554</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RT @L_Carranzaa: if only I was on the plane wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1431</td>\n",
       "      <td>693</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36443</td>\n",
       "      <td>4</td>\n",
       "      <td>43507</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>Begging To You - Marty Robbins http://t.co/Mi8...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>243</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>775617</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>RT @MehrTarar: Remember #AjmalKasab of #Pakist...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>343</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>344</td>\n",
       "      <td>2</td>\n",
       "      <td>3896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>@billmaher Your girlfriend @AnnCoulter is on t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>173</td>\n",
       "      <td>263</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>743</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>RT @CdotDOPE: how the fuck you go missing for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2931</td>\n",
       "      <td>2330</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>586</td>\n",
       "      <td>8</td>\n",
       "      <td>79552</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>“@DaMainManFred: why am I so hungry? ?”</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>279</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5438</td>\n",
       "      <td>5</td>\n",
       "      <td>63988</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>RT @HafizIrfann: Happy Birthday to one of my i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>678</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>615</td>\n",
       "      <td>0</td>\n",
       "      <td>25694</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>Everything is so much better on a sunny day. T...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154</td>\n",
       "      <td>621</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>306</td>\n",
       "      <td>1</td>\n",
       "      <td>1291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>@AliiceLee I know double @leemack on a friday ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>222</td>\n",
       "      <td>662</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2373</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>#Miscellaneous #5: I'm Just Your Yesterday (fe...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248</td>\n",
       "      <td>274</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>17146</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>Why is my weird level cranked up to 2722253493...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>375</td>\n",
       "      <td>380</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4975</td>\n",
       "      <td>5</td>\n",
       "      <td>27157</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>\"@sweetlyblunt: Yes, some things ARE better le...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>910</td>\n",
       "      <td>696</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>216</td>\n",
       "      <td>14</td>\n",
       "      <td>4957</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>@iFawkkedDrizzy and she like looks good with a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>906</td>\n",
       "      <td>746</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90486</td>\n",
       "      <td>16</td>\n",
       "      <td>97205</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>RT @R3DBE_COOLIN: That why you can't take life...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>905</td>\n",
       "      <td>741</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>11</td>\n",
       "      <td>13312</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>This view is the sole reason why I could possi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>326</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2231</td>\n",
       "      <td>2</td>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>Thugs \"@charliebihh: Why are these fools shoot...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1086</td>\n",
       "      <td>730</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>23</td>\n",
       "      <td>231383</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>nowplaying ♫ Just jack - starz in their eyes ♫...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>21700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>RT @iam_allSMiLES: love .. that’s the biggest ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>713</td>\n",
       "      <td>637</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>46665</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>Live! «@AlanaHyland Thank You Mama @_triciamir...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26086</td>\n",
       "      <td>578</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3485</td>\n",
       "      <td>113</td>\n",
       "      <td>33362</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>So, going out tomorrow night with @morgan_hebe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>993</td>\n",
       "      <td>407</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8396</td>\n",
       "      <td>2</td>\n",
       "      <td>35671</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>@EDMsn00b I live in the suburbs. well diff sub...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>649</td>\n",
       "      <td>644</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4233</td>\n",
       "      <td>2</td>\n",
       "      <td>11985</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>RT @SaraParamo: I'm not going home alone, coz ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>304</td>\n",
       "      <td>1245</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1509</td>\n",
       "      <td>1</td>\n",
       "      <td>5115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>Thats on me</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>173</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>1592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>RT @lmaginations: I may not be your first love...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>506</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>159</td>\n",
       "      <td>341</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>3767</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>RT @karencear2: @austinmahone follow me my lov...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11578</td>\n",
       "      <td>1525</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>62</td>\n",
       "      <td>371863</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>do not want to go to college for this interview</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1141</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7101</td>\n",
       "      <td>1</td>\n",
       "      <td>10054</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Masterchef last night hit new highs in sodded ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2392</td>\n",
       "      <td>2825</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>373</td>\n",
       "      <td>78</td>\n",
       "      <td>16298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>#-Do you want to Stop Dieting... to Start Eati...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2216</td>\n",
       "      <td>1898</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>108990</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>RT @ltsTyga: I don't care if it's 5 minutes or...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>193</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>8066</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>@lemahey love you too, you indirect happy freak</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>393</td>\n",
       "      <td>355</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14584</td>\n",
       "      <td>0</td>\n",
       "      <td>7769</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>Miley just got bitch-er. Ew RT @marfahlia: Ew ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>773</td>\n",
       "      <td>492</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2404</td>\n",
       "      <td>3</td>\n",
       "      <td>50316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2001 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0                   1   \\\n",
       "0                                                  text  numberOfHashtags_c   \n",
       "1                 That just seriously ruined my night..                   0   \n",
       "2     @JessleaC I no they are so funny ..! I love yo...                   0   \n",
       "3     @honkwas might drive so we could go subway or ...                   0   \n",
       "4     RT @4dize: my friends keep tellin me, that if ...                   0   \n",
       "5     That @brad_frost post is bourne out by my expe...                   0   \n",
       "6          why tf do i keep waking up every 2 hours -.-                   0   \n",
       "7     I love playing a girl Fav song & watch them go...                   0   \n",
       "8     Yup late bout to write some music. Working on ...                   1   \n",
       "9     RT @2PM_facts: RT @jypnation [2PM LIVE TOUR in...                   0   \n",
       "10    i be up all night whole crew on here. cuz i do...                   0   \n",
       "11    @FGW passenger fainted on 734am bristol glouce...                   0   \n",
       "12    Make a change from other artist and actually t...                   0   \n",
       "13           My 2nd pair of ghds are on there way out??                   0   \n",
       "14    \"@TheBlogz: @K_B_Production nigga. You need yo...                   0   \n",
       "15    @DanielleLopez14 @BritttanyBetchh and now Case...                   1   \n",
       "16                    Where words fails, music Speaks !                   0   \n",
       "17    RT @1Obefiend: So i heard Milo Suam kena tangk...                   0   \n",
       "18    If you ever sleep with @MilaKunis you'd have t...                   0   \n",
       "19    Thankyou so much you beautiful boy. You are th...                   0   \n",
       "20    “@cynthia3950: @ABQuintanilla3 Thank u soo muc...                   0   \n",
       "21    RT @AndieForbes_xx: I love the south African a...                   0   \n",
       "22    RT @WSJ: Today's smartphones have roughly the ...                   0   \n",
       "23    RT @__Skully: I love how angry my brother gets...                   0   \n",
       "24    I've collected 12,293 gold coins! http://t.co/...                   3   \n",
       "25    RT @FIirting: It's amazing how you can fall in...                   0   \n",
       "26    RT @carolekirk: One week to go! 9th May 11-3 t...                   0   \n",
       "27    @HaleyMatthew02 how were u playing on PokerSta...                   0   \n",
       "28    people that nickname their first name on twitt...                   1   \n",
       "29    RT @L_Carranzaa: if only I was on the plane wi...                   0   \n",
       "...                                                 ...                 ...   \n",
       "1971  Begging To You - Marty Robbins http://t.co/Mi8...                   2   \n",
       "1972  RT @MehrTarar: Remember #AjmalKasab of #Pakist...                   2   \n",
       "1973  @billmaher Your girlfriend @AnnCoulter is on t...                   0   \n",
       "1974  RT @CdotDOPE: how the fuck you go missing for ...                   0   \n",
       "1975            “@DaMainManFred: why am I so hungry? ?”                   0   \n",
       "1976  RT @HafizIrfann: Happy Birthday to one of my i...                   0   \n",
       "1977  Everything is so much better on a sunny day. T...                   1   \n",
       "1978  @AliiceLee I know double @leemack on a friday ...                   2   \n",
       "1979  #Miscellaneous #5: I'm Just Your Yesterday (fe...                   3   \n",
       "1980  Why is my weird level cranked up to 2722253493...                   0   \n",
       "1981  \"@sweetlyblunt: Yes, some things ARE better le...                   1   \n",
       "1982  @iFawkkedDrizzy and she like looks good with a...                   0   \n",
       "1983  RT @R3DBE_COOLIN: That why you can't take life...                   0   \n",
       "1984  This view is the sole reason why I could possi...                   0   \n",
       "1985  Thugs \"@charliebihh: Why are these fools shoot...                   0   \n",
       "1986  nowplaying ♫ Just jack - starz in their eyes ♫...                   2   \n",
       "1987  RT @iam_allSMiLES: love .. that’s the biggest ...                   1   \n",
       "1988  Live! «@AlanaHyland Thank You Mama @_triciamir...                   0   \n",
       "1989  So, going out tomorrow night with @morgan_hebe...                   0   \n",
       "1990  @EDMsn00b I live in the suburbs. well diff sub...                   1   \n",
       "1991  RT @SaraParamo: I'm not going home alone, coz ...                   0   \n",
       "1992                                        Thats on me                   0   \n",
       "1993  RT @lmaginations: I may not be your first love...                   0   \n",
       "1994  RT @karencear2: @austinmahone follow me my lov...                   0   \n",
       "1995    do not want to go to college for this interview                   0   \n",
       "1996  Masterchef last night hit new highs in sodded ...                   0   \n",
       "1997  #-Do you want to Stop Dieting... to Start Eati...                   0   \n",
       "1998  RT @ltsTyga: I don't care if it's 5 minutes or...                   0   \n",
       "1999    @lemahey love you too, you indirect happy freak                   0   \n",
       "2000  Miley just got bitch-er. Ew RT @marfahlia: Ew ...                   0   \n",
       "\n",
       "                  2              3              4                   5   \\\n",
       "0     favorite_count  retweet_count  maliciousMark  possibly_sensitive   \n",
       "1                  0              0              0                 NaN   \n",
       "2                  0              0              0                 NaN   \n",
       "3                  0              0              0                 NaN   \n",
       "4                  0              1              0                 NaN   \n",
       "5                  0              0              0                 NaN   \n",
       "6                  0              0              0                 NaN   \n",
       "7                  0              0              0                 NaN   \n",
       "8                  2              1              0                 NaN   \n",
       "9                  0              5              0                 NaN   \n",
       "10                 0              0              0                 NaN   \n",
       "11                 0              0              0                 NaN   \n",
       "12                 0              0              0                   0   \n",
       "13                 0              0              1                 NaN   \n",
       "14                 0              0              0                 NaN   \n",
       "15                 1              0              0                 NaN   \n",
       "16                 0              0              0                 NaN   \n",
       "17                 0            105              0                 NaN   \n",
       "18                 1              1              0                 NaN   \n",
       "19                 0              0              0                 NaN   \n",
       "20                 0              0              0                 NaN   \n",
       "21                 0              2              0                 NaN   \n",
       "22                 0            403              0                   0   \n",
       "23                 0              1              0                 NaN   \n",
       "24                 0              0              1                   0   \n",
       "25                 0           1115              0                 NaN   \n",
       "26                 0              3              0                 NaN   \n",
       "27                 0              0              0                 NaN   \n",
       "28                 0              0              0                 NaN   \n",
       "29                 0              1              0                 NaN   \n",
       "...              ...            ...            ...                 ...   \n",
       "1971               0              0              0                   0   \n",
       "1972               0            168              0                 NaN   \n",
       "1973               0              0              0                 NaN   \n",
       "1974               0              1              0                 NaN   \n",
       "1975               0              0              0                 NaN   \n",
       "1976               0             39              0                 NaN   \n",
       "1977               1              0              0                 NaN   \n",
       "1978               0              1              0                 NaN   \n",
       "1979               0              1              0                   0   \n",
       "1980               0              0              0                 NaN   \n",
       "1981               0              0              0                 NaN   \n",
       "1982               0              0              0                 NaN   \n",
       "1983               0              3              0                 NaN   \n",
       "1984               0              0              0                   0   \n",
       "1985               0              0              0                 NaN   \n",
       "1986               0              0              0                   0   \n",
       "1987               0              1              0                 NaN   \n",
       "1988               0              0              0                 NaN   \n",
       "1989               0              1              0                 NaN   \n",
       "1990               1              0              0                 NaN   \n",
       "1991               0              1              0                 NaN   \n",
       "1992               0              0              0                 NaN   \n",
       "1993               0            506              0                 NaN   \n",
       "1994               0              1              1                 NaN   \n",
       "1995               0              0              0                 NaN   \n",
       "1996               0              0              0                 NaN   \n",
       "1997               0              0              1                   0   \n",
       "1998               0              0              0                 NaN   \n",
       "1999               0              0              0                 NaN   \n",
       "2000               0              1              0                   0   \n",
       "\n",
       "                   6              7                8                      9   \\\n",
       "0     followers_count  friends_count  default_profile  default_profile_image   \n",
       "1                 173             94                0                      0   \n",
       "2                 129            202                1                      0   \n",
       "3                 207            372                1                      0   \n",
       "4                6218            915                0                      0   \n",
       "5                 282            471                0                      0   \n",
       "6                 836            415                0                      0   \n",
       "7                3301           1113                0                      0   \n",
       "8                 444            331                0                      0   \n",
       "9                 451            277                0                      0   \n",
       "10                 94            233                1                      0   \n",
       "11                 87            271                1                      0   \n",
       "12              79222          37027                0                      0   \n",
       "13                334            278                0                      0   \n",
       "14               1426            647                1                      0   \n",
       "15                 76             77                0                      0   \n",
       "16                  6            172                0                      0   \n",
       "17                886            587                0                      0   \n",
       "18               1067           1742                0                      0   \n",
       "19                727            967                0                      0   \n",
       "20                 88           1072                1                      0   \n",
       "21                368            184                0                      0   \n",
       "22                 30            304                0                      0   \n",
       "23                742            596                0                      0   \n",
       "24                 10             19                1                      0   \n",
       "25                416            396                0                      0   \n",
       "26               2027           2471                1                      0   \n",
       "27               1340            867                1                      0   \n",
       "28                971            696                0                      0   \n",
       "29               1431            693                1                      0   \n",
       "...               ...            ...              ...                    ...   \n",
       "1971              243              0                0                      1   \n",
       "1972              343            307                0                      0   \n",
       "1973              173            263                0                      0   \n",
       "1974             2931           2330                0                      0   \n",
       "1975              279            178                0                      0   \n",
       "1976              678            280                0                      0   \n",
       "1977              154            621                0                      0   \n",
       "1978              222            662                1                      0   \n",
       "1979              248            274                0                      0   \n",
       "1980              375            380                0                      0   \n",
       "1981              910            696                0                      0   \n",
       "1982              906            746                0                      0   \n",
       "1983              905            741                0                      0   \n",
       "1984               38            326                1                      0   \n",
       "1985             1086            730                0                      0   \n",
       "1986               26              8                1                      0   \n",
       "1987              713            637                0                      0   \n",
       "1988            26086            578                0                      0   \n",
       "1989              993            407                0                      0   \n",
       "1990              649            644                0                      0   \n",
       "1991              304           1245                1                      0   \n",
       "1992              173            155                1                      0   \n",
       "1993              159            341                0                      0   \n",
       "1994            11578           1525                0                      0   \n",
       "1995             1141            164                0                      0   \n",
       "1996             2392           2825                1                      0   \n",
       "1997             2216           1898                0                      0   \n",
       "1998              193            133                0                      0   \n",
       "1999              393            355                0                      0   \n",
       "2000              773            492                0                      0   \n",
       "\n",
       "                    10            11              12        13  \n",
       "0     favourites_count  listed_count  statuses_count  verified  \n",
       "1                  562             2           22545         0  \n",
       "2                 1131             1            2192         0  \n",
       "3                 1066             1            3426         0  \n",
       "4                  513            44          223046         0  \n",
       "5                 5452            54            5378         0  \n",
       "6                 1814             2           12523         0  \n",
       "7                   60             9           26644         0  \n",
       "8                11964             3           10852         0  \n",
       "9                    9             1           24706         0  \n",
       "10                  49             2            1469         0  \n",
       "11                  40             3            1189         0  \n",
       "12                2144           142           56272         0  \n",
       "13                2531             0           17858         0  \n",
       "14                6555            13           30125         0  \n",
       "15                2314             0            2742         0  \n",
       "16                  10             0              39         0  \n",
       "17                 730             9           82058         0  \n",
       "18                1430            14            8877         0  \n",
       "19                5610             0           20205         0  \n",
       "20                 349             1            1245         0  \n",
       "21                1061             0           32569         0  \n",
       "22                  18             1              29         0  \n",
       "23                5777             0           11320         0  \n",
       "24                   0             0           13134         0  \n",
       "25                9666             1           52208         0  \n",
       "26                 831            85            8020         0  \n",
       "27                1233            24           22959         0  \n",
       "28               10110             4           10554         0  \n",
       "29               36443             4           43507         0  \n",
       "...                ...           ...             ...       ...  \n",
       "1971                 0            21          775617         0  \n",
       "1972               344             2            3896         0  \n",
       "1973               139             9             743         0  \n",
       "1974               586             8           79552         0  \n",
       "1975              5438             5           63988         0  \n",
       "1976               615             0           25694         0  \n",
       "1977               306             1            1291         0  \n",
       "1978                 2             6            2373         0  \n",
       "1979                 0            72           17146         0  \n",
       "1980              4975             5           27157         0  \n",
       "1981               216            14            4957         0  \n",
       "1982             90486            16           97205         0  \n",
       "1983                84            11           13312         0  \n",
       "1984              2231             2             866         0  \n",
       "1985                49            23          231383         0  \n",
       "1986                 0             4           21700         0  \n",
       "1987                37             5           46665         0  \n",
       "1988              3485           113           33362         0  \n",
       "1989              8396             2           35671         0  \n",
       "1990              4233             2           11985         0  \n",
       "1991              1509             1            5115         0  \n",
       "1992               185             1            1592         0  \n",
       "1993              1987             0            3767         0  \n",
       "1994               137            62          371863         0  \n",
       "1995              7101             1           10054         0  \n",
       "1996               373            78           16298         0  \n",
       "1997                 0            38          108990         0  \n",
       "1998                10             0            8066         0  \n",
       "1999             14584             0            7769         0  \n",
       "2000              2404             3           50316         0  \n",
       "\n",
       "[2001 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df[0]) # 15 columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNames = list(df[0].loc[0])\n",
    "df = df[0].iloc[1:, :]\n",
    "df.columns = columnNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, Y_train, Y_test = train_test_split(df.drop('maliciousMark', axis = 1), df['maliciousMark'], test_size=args.validation_portion , random_state=args.random_seed)\n",
    "# X_validation, X_test, Y_validation, Y_test = train_test_split(X_test, Y_test, test_size=args.test_portion, random_state=args.random_seed)\n",
    "\n",
    "# # stratify=df['maliciousMark'], stratify=Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "def preprocessingInputTextData(colName):\n",
    "    input = df[colName]\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    tknzr = TweetTokenizer()\n",
    "    allText = [i for i in input]\n",
    "    preprocessedText = [[ps.stem(word) for word in tknzr.tokenize(re.sub(r'\\d+', '', re.sub(r\"http\\S+|www.\\S+\", matchingURL,\n",
    "                                                                                            sentence)).lower()) if word not in nltk.corpus.stopwords.words('english') and len(word) >= 3] for sentence in allText]\n",
    "    df[colName] = preprocessedText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should I use the inplace method or the return?\n",
    "\n",
    "def fillingNullValue(colName):   \n",
    "    if preprocessingStra[colName]['fillingNullMethod'] == filling_method.MOST_COMMON:\n",
    "        df[colName] = df[colName].astype('float')\n",
    "        df[colName].fillna(df[colName].mean(), inplace=True)\n",
    "    elif preprocessingStra[colName]['fillingNullMethod'] == filling_method.MEAN:\n",
    "        df[colName] = df[colName].astype('category')\n",
    "        df[colName].fillna(df[colName].astype(\n",
    "            'category').describe()['top'], inplace=True)\n",
    "    elif preprocessingStra[colName]['fillingNullMethod'] == filling_method.CERTAIN_VALUE:\n",
    "        df[colName] = df[colName].astype('category')\n",
    "        df[colName] = df[colName].cat.add_categories(\n",
    "            [preprocessingStra[colName]['fillingNullValue']])\n",
    "        df[colName].fillna(preprocessingStra[colName]\n",
    "                           ['fillingNullValue'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class filling_method():\n",
    "    MOST_COMMON = \"MOST_COMMON\"\n",
    "    MEAN = \"MEAN\"\n",
    "    CERTAIN_VALUE = \"CERTAIN_VALUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TweetsWithUserInfoPreprocessing():\n",
    "    for colName in preprocessingStra.keys():\n",
    "        for step in preprocessingStra[colName]['steps']:\n",
    "            print(colName, ':', step)\n",
    "            if not step is None:\n",
    "                step(colName)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessingStra = defaultdict(dict)\n",
    "preprocessingStra['text']['steps'] = [preprocessingInputTextData]\n",
    "preprocessingStra[\"numberOfHashtags_c\"]['steps']  = [None]\n",
    "preprocessingStra['favorite_count']['steps'] = [None]\n",
    "preprocessingStra['retweet_count']['steps'] = [None]\n",
    "preprocessingStra['possibly_sensitive'] = {\n",
    "    'fillingNullMethod': filling_method.CERTAIN_VALUE,\n",
    "    'fillingNullValue': 'UNKNOWN',\n",
    "    'steps': [fillingNullValue],\n",
    "}\n",
    "preprocessingStra['followers_count']['steps'] = [None]\n",
    "preprocessingStra['friends_count']['steps'] = [None]\n",
    "# preprocessingStra['description']= {\n",
    "#    'steps': [ fillingNullValue,preprocessingInputTextData],\n",
    "#     'fillingNullMethod': filling_method.CERTAIN_VALUE,\n",
    "#     'fillingNullValue': 'NULLDescription'\n",
    "# } \n",
    "preprocessingStra['default_profile']['steps'] = [None]\n",
    "preprocessingStra['default_profile_image']['steps'] = [None]\n",
    "preprocessingStra['favourites_count']['steps'] = [None]\n",
    "preprocessingStra['listed_count']['steps'] = [None]\n",
    "preprocessingStra['statuses_count']['steps'] =[None] \n",
    "preprocessingStra['verified']['steps'] = [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text : <function preprocessingInputTextData at 0x7fec126267b8>\n",
      "numberOfHashtags_c : None\n",
      "favorite_count : None\n",
      "retweet_count : None\n",
      "possibly_sensitive : <function fillingNullValue at 0x7fec12626488>\n",
      "followers_count : None\n",
      "friends_count : None\n",
      "default_profile : None\n",
      "default_profile_image : None\n",
      "favourites_count : None\n",
      "listed_count : None\n",
      "statuses_count : None\n",
      "verified : None\n"
     ]
    }
   ],
   "source": [
    "newdf = TweetsWithUserInfoPreprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, drop_first = True, columns=['possibly_sensitive', 'default_profile', 'default_profile_image', 'verified'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df.drop('maliciousMark', axis = 1), df['maliciousMark'], test_size=args.validation_portion ,stratify=df['maliciousMark'],  random_state=args.random_seed)\n",
    "X_validation, X_test, Y_validation, Y_test = train_test_split(X_test, Y_test, test_size=args.test_portion, stratify=Y_test,random_state=args.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1900"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_text = nltk.Text(list(itertools.chain(*X_train['text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.tweets_vocab_size = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Constants import specialTokenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.tweets_vocab_size:  \n",
    "    tweets_text.tokens = specialTokenList + \\\n",
    "        [w for w, _ in tweets_text.vocab().most_common(\n",
    "            args.tweets_vocab_size - len(specialTokenList))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_text.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapFromWordToIdx(input, text):\n",
    "    '''\n",
    "    Using text, so it will be changed once we update the text\n",
    "    '''\n",
    "    wholeText = []\n",
    "\n",
    "    for s in input:\n",
    "        sentence = [2]\n",
    "        for w in s:\n",
    "            if w in text.tokens:\n",
    "                sentence.append(text.index(w))\n",
    "            else:\n",
    "                sentence.append(1)\n",
    "        sentence.append(3)\n",
    "        wholeText.append(sentence)\n",
    "\n",
    "    return wholeText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['text'] = mapFromWordToIdx(X_train['text'], tweets_text)\n",
    "X_validation['text'] = mapFromWordToIdx(X_validation['text'], tweets_text)\n",
    "X_test['text'] = mapFromWordToIdx(X_test['text'], tweets_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'numberOfHashtags_c', 'favorite_count', 'retweet_count',\n",
       "       'maliciousMark', 'followers_count', 'friends_count', 'favourites_count',\n",
       "       'listed_count', 'statuses_count', 'possibly_sensitive_1',\n",
       "       'possibly_sensitive_UNKNOWN', 'default_profile_1',\n",
       "       'default_profile_image_1', 'verified_1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetsWithUserInfoDataset(Dataset):\n",
    "    def __init__(self, text_p, X_ordered, text_len, y_ordered):\n",
    "        super(TweetsWithUserInfoDataset, self).__init__()\n",
    "\n",
    "        self.text_p = text_p\n",
    "        self.X_ordered = X_ordered\n",
    "        self.text_len = text_len\n",
    "        self.y_ordered = y_ordered\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.y_ordered)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text_p[idx], self.X_ordered[idx], self.text_len[idx], self.y_ordered[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateTweetsWithUserInfoDatatset(X, y):\n",
    "    '''\n",
    "    At these step two kind of the sequential data has to be coped with, twitter text and \n",
    "    '''\n",
    "    text_len = torch.tensor(list(map(len, X['text'])))\n",
    "    text_len, text_len_idx = text_len.sort(0, descending=True)\n",
    "    \n",
    "    text_ordered = []\n",
    "    text_ordered = [torch.LongTensor(list(X['text'])[i.item()]) for i in text_len_idx]\n",
    "    X_ordered = torch.FloatTensor([list(map(float, list(X.drop(['text'], axis = 1).iloc[i.item(),:]))) for i in text_len_idx])\n",
    "    y_ordered = torch.FloatTensor(np.array([y[i] for i in text_len_idx]))\n",
    "    text_p = pad_sequence(text_ordered, batch_first=True)\n",
    "    dataset = TweetsWithUserInfoDataset(text_p, X_ordered, text_len, y_ordered)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = CreateTweetsWithUserInfoDatatset(X_train, list(map(int, list(Y_train))))\n",
    "validation_dataset = CreateTweetsWithUserInfoDatatset(X_validation, list(map(int, list(Y_validation))))\n",
    "test_dataset = CreateTweetsWithUserInfoDatatset(X_test, list(map(int, list(Y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, textModel, args):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        \n",
    "        self.textModel = textModel(args)\n",
    "        \n",
    "        self.infoModel = nn.Sequential(\n",
    "            nn.Linear(args.num_features, args.FC_hidden),\n",
    "            nn.BatchNorm1d(args.FC_hidden),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Linear(args.FC_hidden, args.FC_hidden),\n",
    "            nn.BatchNorm1d(args.FC_hidden),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Linear(args.FC_hidden, args.FC_hidden),\n",
    "            nn.BatchNorm1d(args.FC_hidden),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Linear(args.FC_hidden, 1),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, input, length):\n",
    "        \n",
    "        \n",
    "        text_input, extra_info = input\n",
    "        \n",
    "        text_out = self.textModel(text_input, length)\n",
    "        \n",
    "        return self.infoModel(torch.cat((text_out, extra_info), dim = 1))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from All_Models import GatedCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Spammer:  tensor(196.)\n",
      "Number of NonSpammer:  tensor(1704.)\n"
     ]
    }
   ],
   "source": [
    "# Put it after the training set\n",
    "args.numberOfSpammer = sum([t[-1] for t in training_dataset])\n",
    "args.numberOfNonSpammer = len(training_dataset)-args.numberOfSpammer\n",
    "args.len_max_seq = training_dataset[0][2]\n",
    "\n",
    "print(\"Number of Spammer: \", args.numberOfSpammer)\n",
    "print(\"Number of NonSpammer: \", args.numberOfNonSpammer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "\n",
    "class MultiTaskTrainer(nn.Module):\n",
    "\n",
    "    def __init__(self, model, args):\n",
    "        super(MultiTaskTrainer, self).__init__()\n",
    "\n",
    "        self.model = MultiTaskModel(model, args)\n",
    "        self.optim = optim.Adam(\n",
    "            self.model.parameters(), lr=args.lr, weight_decay=args.L2)\n",
    "\n",
    "        if args.usingWeightRandomSampling:\n",
    "            pos_weight = None\n",
    "        else:\n",
    "            pos_weight = torch.tensor(\n",
    "                args.numberOfNonSpammer/args.numberOfSpammer)\n",
    "\n",
    "        self.threshold = args.threshold\n",
    "        self.log_path = args.log_path\n",
    "        self.model_path = args.model_path\n",
    "        self.model_name = args.model_name\n",
    "\n",
    "        self.Loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        self.hist = defaultdict(list)\n",
    "        self.cms = defaultdict(list)\n",
    "        self.confusion_matrics = []\n",
    "        \n",
    "        self.apply(self.weight_init)\n",
    "\n",
    "\n",
    "    def forward(self, input, label):\n",
    "\n",
    "        if type(input) is tuple:\n",
    "            input, lengths = input\n",
    "        else:\n",
    "            lengths = None\n",
    "\n",
    "        self.pred = self.model(input, lengths)\n",
    "        self.label = label\n",
    "        loss = self.Loss(self.pred.squeeze(1), label)\n",
    "\n",
    "        accuracy = torch.mean(\n",
    "            ((torch.sigmoid(self.pred) > self.threshold).squeeze(-1) == label.byte()).float())\n",
    "\n",
    "        cm = confusion_matrix(label.cpu().numpy(),\n",
    "                              (torch.sigmoid(self.pred) > self.threshold).cpu().numpy())\n",
    "\n",
    "        return loss, accuracy, cm\n",
    "\n",
    "    def train_step(self, input, label):\n",
    "\n",
    "        self.optim.zero_grad()\n",
    "\n",
    "        self.loss, self.accuracy, self.cm = self.forward(input, label)\n",
    "\n",
    "        self.hist[\"Temp_Train_Loss\"].append(self.loss.item())\n",
    "        self.hist[\"Temp_Train_Accuracy\"].append(self.accuracy.item())\n",
    "        self.hist[\"Train_Loss\"].append(self.loss.item())\n",
    "        self.hist[\"Train_Accuracy\"].append(self.accuracy.item())\n",
    "        self.cms[\"Train\"].append(self.cm)\n",
    "        self.cms[\"Train\"] = self.cms[\"Train\"][-10:]\n",
    "        self.loss.backward()\n",
    "        # clip_grad_norm(self.model.parameters(), 0.25)\n",
    "        self.optim.step()\n",
    "\n",
    "        return self.loss, self.accuracy, self.cm\n",
    "\n",
    "    def test_step(self, input, label, validation=True):\n",
    "\n",
    "        # Not Updating the weight\n",
    "\n",
    "        self.loss, self.accuracy, self.cm = self.forward(input, label)\n",
    "\n",
    "        if validation:\n",
    "            self.hist[\"Temp_Val_Loss\"].append(self.loss.item())\n",
    "            self.hist[\"Temp_Val_Accuracy\"].append(self.accuracy.item())\n",
    "            self.hist[\"Val_Loss\"].append(self.loss.item())\n",
    "            self.hist[\"Val_Accuracy\"].append(self.accuracy.item())\n",
    "            self.cms[\"Val\"].append(self.cm)\n",
    "            self.cms[\"Val\"] = self.cms[\"Val\"][-10:]\n",
    "        else:\n",
    "            self.hist[\"Temp_Test_Loss\"].append(self.loss.item())\n",
    "            self.hist[\"Temp_Test_Accuracy\"].append(self.accuracy.item())\n",
    "            self.hist[\"Test_Loss\"].append(self.loss.item())\n",
    "            self.hist[\"Test_Accuracy\"].append(self.accuracy.item())\n",
    "            self.cms[\"Test\"].append(self.cm)\n",
    "            self.cms[\"Test\"] = self.cms[\"Test\"][-10:]\n",
    "\n",
    "        return self.loss, self.accuracy, self.cm\n",
    "\n",
    "    def calculateAverage(self,):\n",
    "\n",
    "        temp_keys = deepcopy(list(self.hist.keys()))\n",
    "        for name in temp_keys:\n",
    "            if 'Temp' in name:\n",
    "                self.hist[\"Average\" + name[4:]\n",
    "                          ].append(sum(self.hist[name])/len(self.hist[name]))\n",
    "                self.hist[name] = []\n",
    "\n",
    "    def plot_train_hist(self, step):\n",
    "\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        num_loss = 2\n",
    "        i = 0\n",
    "        for name in self.hist.keys():\n",
    "            if 'Train' in name and not \"Temp\" in name and not \"Average\" in name:\n",
    "                i += 1\n",
    "                fig.add_subplot(num_loss, 1, i)\n",
    "                plt.plot(self.hist[name], label=name)\n",
    "                plt.xlabel('Number of Steps', fontsize=15)\n",
    "                plt.ylabel(name, fontsize=15)\n",
    "                plt.title(name, fontsize=30, fontweight=\"bold\")\n",
    "                plt.legend(loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        fig.savefig(self.log_path+\"Train_Loss&Acc_Hist_\"+str(step)+\".png\")\n",
    "\n",
    "    def plot_all(self, step=None):\n",
    "\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        for name in self.hist.keys():\n",
    "            if \"Average\" in name:\n",
    "                if 'Loss' in name:\n",
    "                    plt.subplot(211)\n",
    "                    plt.plot(self.hist[name], marker='o', label=name)\n",
    "                    plt.ylabel('Loss', fontsize=15)\n",
    "                    plt.xlabel('Number of epochs', fontsize=15)\n",
    "                    plt.title('Loss', fontsize=20, fontweight=\"bold\")\n",
    "                    plt.legend(loc='upper left')\n",
    "                if \"Accuracy\" in name:\n",
    "                    plt.subplot(212)\n",
    "                    plt.plot(self.hist[name], marker='o', label=name)\n",
    "                    plt.ylabel('Accuracy', fontsize=15)\n",
    "                    plt.xlabel('Number of epochs', fontsize=15)\n",
    "                    plt.title('Accuracy', fontsize=20, fontweight=\"bold\")\n",
    "                    plt.legend(loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        if step is not None:\n",
    "            fig.savefig(self.log_path + \"All_Hist_\"+str(step)+\".png\")\n",
    "\n",
    "    def model_save(self, step):\n",
    "\n",
    "        path = self.model_path + self.model_name+'_Step_' + str(step) + '.pth'\n",
    "        torch.save({self.model_name: self.state_dict()}, path)\n",
    "        print('Model Saved')\n",
    "\n",
    "    def load_step_dict(self, step):\n",
    "\n",
    "        path = self.model_path + self.model_name + \\\n",
    "            '_Step_' + str(step) + '.pth'\n",
    "        self.load_state_dict(torch.load(\n",
    "            path, map_location=lambda storage, loc: storage)[self.model_name])\n",
    "        print('Model Loaded')\n",
    "\n",
    "    def num_all_params(self,):\n",
    "        return sum([param.nelement() for param in self.parameters()])\n",
    "    \n",
    "    \n",
    "    def weight_init(self, m):\n",
    "\n",
    "        if type(m) in [nn.Conv2d, nn.ConvTranspose2d, nn.Linear, nn.Conv1d]:\n",
    "            nn.init.kaiming_normal_(m.weight, 0.2, nonlinearity='leaky_relu')\n",
    "        elif type(m) in [nn.LSTM]:\n",
    "            for name, value in m.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_normal_(value.data)\n",
    "                if 'bias'in name:\n",
    "                    value.data.normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_extra_info = len(X_train.drop(['text'], axis = 1).columns)\n",
    "args.num_features = len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chihcheng/python3env/lib/python3.6/site-packages/torch/utils/data/sampler.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.weights = torch.tensor(weights, dtype=torch.double)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-f59ff33f1cea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     validation_dataset, batch_size=args.batch_size, shuffle=True, drop_last=False)\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatedCNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of Parameters in this Model: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_all_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Trainer' is not defined"
     ]
    }
   ],
   "source": [
    "if args.usingWeightRandomSampling:\n",
    "    sampler = getSampler(training_dataset)\n",
    "else:\n",
    "    sampler = None\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    training_dataset, batch_size=args.batch_size, shuffle=False, drop_last=False, sampler = sampler)\n",
    "valid_loader = DataLoader(\n",
    "    validation_dataset, batch_size=args.batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "trainer = Trainer(GatedCNN, args).to(device)\n",
    "\n",
    "print(\"Number of Parameters in this Model: \",trainer.num_all_params())\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(trainer.optim, 2000, gamma=0.85)\n",
    "# trainer.optim.param_groups[0]['lr']=\n",
    "allStep = 0\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Structure: \\n\", trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while epoch < args.n_epoch:\n",
    "    for i, (text, extra_info, length, label) in enumerate(train_loader):\n",
    "        \n",
    "        trainer.train()\n",
    "        text, extra_info, length, label = text.to(device), extra_info.to(device), length.to(device), label.to(device)\n",
    "        \n",
    "        if trainer.optim.param_groups[0]['lr'] >= 0.00001:\n",
    "            scheduler.step()\n",
    "        start_t = time.time()\n",
    "        trainer.train_step(((text, extra_info),length), label)\n",
    "#         trainer.train_step((text, extra_info), label)\n",
    "\n",
    "        end_t = time.time()\n",
    "        allStep += 1\n",
    "        print('| Epoch [%d] | Step [%d] | lr [%.6f] | Loss: [%.4f] | Acc: [%.4f] | Time: %.1fs' %\n",
    "              (epoch, allStep, trainer.optim.param_groups[0]['lr'], trainer.loss.item(), trainer.accuracy.item(),\n",
    "               end_t - start_t))\n",
    "\n",
    "#         if trainer.accuracy.item() > 0.95: # Stop early\n",
    "#             raise StopIteration\n",
    "        if allStep % args.log_freq == 0:\n",
    "            trainer.plot_train_hist(args.model_name)\n",
    "            \n",
    "        \n",
    "        if args.earlyStopStep:\n",
    "            if allStep >= args.earlyStopStep:\n",
    "                    raise StopIteration\n",
    "        \n",
    "\n",
    "        if allStep % args.val_freq == 0:\n",
    "\n",
    "            for _ in range(args.val_steps):\n",
    "                trainer.eval()\n",
    "                stIdx = np.random.randint(\n",
    "                    0, len(validation_dataset) - args.batch_size)\n",
    "                v_text, v_extra_info, v_len, v_label = validation_dataset[stIdx: stIdx +\n",
    "                                                       args.batch_size]\n",
    "                v_text, v_extra_info, v_len, v_label = v_text.to(device), v_extra_info.to(device), v_len.to(device), v_label.to(device)\n",
    "                start_t = time.time()\n",
    "                trainer.test_step(((v_text, v_extra_info),v_len), v_label)\n",
    "#                 trainer.test_step((v_text, v_extra_info), v_label)\n",
    "                end_t = time.time()\n",
    "                print('| Epoch [%d] | Validation | Step [%d] |  Loss: [%.4f] | Acc: [%.4f] | Time: %.1fs' %\n",
    "                      (epoch, allStep, trainer.loss.item(), trainer.accuracy.item(), end_t - start_t))\n",
    "            trainer.calculateAverage()\n",
    "            clear_output()\n",
    "            print(\"TrainConfusion Matrix: \\n\")\n",
    "            display(pd.DataFrame(trainer.cms['Train'][-1]))\n",
    "            print(\"ValConfusion Matrix: \\n\")\n",
    "            display(pd.DataFrame(trainer.cms['Val'][-1]))\n",
    "            trainer.plot_all(args.model_name)\n",
    "            \n",
    "            \n",
    "     # After every Epoch, if can be moved\n",
    "\n",
    "    epoch += 1\n",
    "    trainer.model_save(epoch)\n",
    "\n",
    "\n",
    "    if args.earlyStopEpoch:\n",
    "        if epoch >= args.earlyStopEpoch:\n",
    "            raise StopIteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text, test_extra_info, test_length, test_label  =  zip(test_dataset[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text, test_extra_info, test_length, test_label  =  zip(test_dataset[0:])\n",
    "test_text, test_extra_info, test_length, test_label  = test_text[0].to(device), test_extra_info[0].to(device), test_length[0].to(device), test_label[0].to(device)\n",
    "\n",
    "trainer.eval()\n",
    "test_loss, test_accuracy, test_cm = trainer.test_step(((test_text, test_extra_info),test_length), test_label)\n",
    "\n",
    "print(\"The Test Loss: \", test_loss.item())\n",
    "print(\"The Test Accuracy: \", test_accuracy.item())\n",
    "print(\"Test Confusion Matrix: \\n\", test_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
